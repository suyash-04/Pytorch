{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990409cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cb58278b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>texture_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>smoothness_se</th>\n",
       "      <th>compactness_se</th>\n",
       "      <th>concavity_se</th>\n",
       "      <th>concave points_se</th>\n",
       "      <th>symmetry_se</th>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
       "0    842302         M  ...                  0.11890          NaN\n",
       "1    842517         M  ...                  0.08902          NaN\n",
       "2  84300903         M  ...                  0.08758          NaN\n",
       "3  84348301         M  ...                  0.17300          NaN\n",
       "4  84358402         M  ...                  0.07678          NaN\n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1d92a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 32', 'id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bd3d388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2, random_state=42, stratify=df.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "89a348bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c36c44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "36137b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6cc88aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 30])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "871d3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.loss = nn.BCELoss()\n",
    " \n",
    "    def forward(self, features):\n",
    "        out = self.linear(features)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    def loss(self, pred, target):\n",
    "        return self.loss(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "460bdb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4648],\n",
       "        [0.3845],\n",
       "        [0.5337],\n",
       "        [0.5370],\n",
       "        [0.3263],\n",
       "        [0.5576],\n",
       "        [0.4983],\n",
       "        [0.5277],\n",
       "        [0.4077],\n",
       "        [0.3362],\n",
       "        [0.3597],\n",
       "        [0.3502],\n",
       "        [0.4963],\n",
       "        [0.4919],\n",
       "        [0.4357],\n",
       "        [0.4568],\n",
       "        [0.4815],\n",
       "        [0.5258],\n",
       "        [0.4593],\n",
       "        [0.4285],\n",
       "        [0.3661],\n",
       "        [0.5052],\n",
       "        [0.4214],\n",
       "        [0.5731],\n",
       "        [0.6068],\n",
       "        [0.4032],\n",
       "        [0.5352],\n",
       "        [0.3873],\n",
       "        [0.3665],\n",
       "        [0.4364],\n",
       "        [0.4542],\n",
       "        [0.5792],\n",
       "        [0.7975],\n",
       "        [0.3574],\n",
       "        [0.5093],\n",
       "        [0.4196],\n",
       "        [0.5308],\n",
       "        [0.4028],\n",
       "        [0.4162],\n",
       "        [0.4495],\n",
       "        [0.4690],\n",
       "        [0.4788],\n",
       "        [0.4845],\n",
       "        [0.3992],\n",
       "        [0.4876],\n",
       "        [0.4327],\n",
       "        [0.5794],\n",
       "        [0.4547],\n",
       "        [0.4008],\n",
       "        [0.3365],\n",
       "        [0.3886],\n",
       "        [0.3969],\n",
       "        [0.4065],\n",
       "        [0.4303],\n",
       "        [0.4107],\n",
       "        [0.3662],\n",
       "        [0.5664],\n",
       "        [0.4568],\n",
       "        [0.4148],\n",
       "        [0.4889],\n",
       "        [0.5381],\n",
       "        [0.3894],\n",
       "        [0.5113],\n",
       "        [0.4064],\n",
       "        [0.4705],\n",
       "        [0.3927],\n",
       "        [0.5378],\n",
       "        [0.5172],\n",
       "        [0.4218],\n",
       "        [0.4422],\n",
       "        [0.5010],\n",
       "        [0.4577],\n",
       "        [0.5864],\n",
       "        [0.4693],\n",
       "        [0.5053],\n",
       "        [0.5416],\n",
       "        [0.4872],\n",
       "        [0.4794],\n",
       "        [0.4989],\n",
       "        [0.5805],\n",
       "        [0.5200],\n",
       "        [0.3987],\n",
       "        [0.4814],\n",
       "        [0.5685],\n",
       "        [0.4204],\n",
       "        [0.4737],\n",
       "        [0.3482],\n",
       "        [0.4412],\n",
       "        [0.5172],\n",
       "        [0.5164],\n",
       "        [0.4762],\n",
       "        [0.4508],\n",
       "        [0.5013],\n",
       "        [0.4806],\n",
       "        [0.5181],\n",
       "        [0.3088],\n",
       "        [0.5123],\n",
       "        [0.4641],\n",
       "        [0.4566],\n",
       "        [0.5388],\n",
       "        [0.5648],\n",
       "        [0.4574],\n",
       "        [0.5147],\n",
       "        [0.4003],\n",
       "        [0.4338],\n",
       "        [0.3170],\n",
       "        [0.4338],\n",
       "        [0.3960],\n",
       "        [0.5283],\n",
       "        [0.3325],\n",
       "        [0.3508],\n",
       "        [0.4776],\n",
       "        [0.3984],\n",
       "        [0.4273],\n",
       "        [0.5549],\n",
       "        [0.4879],\n",
       "        [0.3143],\n",
       "        [0.4283],\n",
       "        [0.4743],\n",
       "        [0.5221],\n",
       "        [0.6310],\n",
       "        [0.4948],\n",
       "        [0.4479],\n",
       "        [0.5749],\n",
       "        [0.4727],\n",
       "        [0.4349],\n",
       "        [0.2786],\n",
       "        [0.4261],\n",
       "        [0.5709],\n",
       "        [0.4472],\n",
       "        [0.6024],\n",
       "        [0.4045],\n",
       "        [0.5547],\n",
       "        [0.5242],\n",
       "        [0.4619],\n",
       "        [0.4666],\n",
       "        [0.4393],\n",
       "        [0.7016],\n",
       "        [0.4421],\n",
       "        [0.4542],\n",
       "        [0.5141],\n",
       "        [0.5918],\n",
       "        [0.5203],\n",
       "        [0.4678],\n",
       "        [0.6909],\n",
       "        [0.3559],\n",
       "        [0.4839],\n",
       "        [0.3675],\n",
       "        [0.6117],\n",
       "        [0.5590],\n",
       "        [0.4812],\n",
       "        [0.4654],\n",
       "        [0.4432],\n",
       "        [0.5638],\n",
       "        [0.5408],\n",
       "        [0.4434],\n",
       "        [0.5198],\n",
       "        [0.3688],\n",
       "        [0.4547],\n",
       "        [0.3989],\n",
       "        [0.4132],\n",
       "        [0.4692],\n",
       "        [0.4818],\n",
       "        [0.4110],\n",
       "        [0.4768],\n",
       "        [0.4869],\n",
       "        [0.4831],\n",
       "        [0.5268],\n",
       "        [0.5126],\n",
       "        [0.5146],\n",
       "        [0.2677],\n",
       "        [0.5203],\n",
       "        [0.4068],\n",
       "        [0.4731],\n",
       "        [0.5061],\n",
       "        [0.4522],\n",
       "        [0.4850],\n",
       "        [0.4507],\n",
       "        [0.3577],\n",
       "        [0.4213],\n",
       "        [0.4593],\n",
       "        [0.4293],\n",
       "        [0.5616],\n",
       "        [0.4541],\n",
       "        [0.5476],\n",
       "        [0.4505],\n",
       "        [0.4720],\n",
       "        [0.4762],\n",
       "        [0.3896],\n",
       "        [0.4063],\n",
       "        [0.4780],\n",
       "        [0.4554],\n",
       "        [0.6321],\n",
       "        [0.4773],\n",
       "        [0.5619],\n",
       "        [0.4256],\n",
       "        [0.4135],\n",
       "        [0.4262],\n",
       "        [0.4732],\n",
       "        [0.4891],\n",
       "        [0.4216],\n",
       "        [0.3807],\n",
       "        [0.4479],\n",
       "        [0.4743],\n",
       "        [0.4297],\n",
       "        [0.4589],\n",
       "        [0.4779],\n",
       "        [0.4574],\n",
       "        [0.5713],\n",
       "        [0.0948],\n",
       "        [0.4375],\n",
       "        [0.5825],\n",
       "        [0.4240],\n",
       "        [0.5582],\n",
       "        [0.4866],\n",
       "        [0.5685],\n",
       "        [0.4319],\n",
       "        [0.4421],\n",
       "        [0.5364],\n",
       "        [0.2682],\n",
       "        [0.5228],\n",
       "        [0.4364],\n",
       "        [0.5235],\n",
       "        [0.3163],\n",
       "        [0.3996],\n",
       "        [0.4226],\n",
       "        [0.4225],\n",
       "        [0.5215],\n",
       "        [0.4274],\n",
       "        [0.5129],\n",
       "        [0.4137],\n",
       "        [0.3938],\n",
       "        [0.5563],\n",
       "        [0.4642],\n",
       "        [0.4632],\n",
       "        [0.5142],\n",
       "        [0.4578],\n",
       "        [0.7250],\n",
       "        [0.4190],\n",
       "        [0.4663],\n",
       "        [0.4523],\n",
       "        [0.5257],\n",
       "        [0.4890],\n",
       "        [0.4593],\n",
       "        [0.4595],\n",
       "        [0.4712],\n",
       "        [0.6017],\n",
       "        [0.4391],\n",
       "        [0.4987],\n",
       "        [0.4356],\n",
       "        [0.4621],\n",
       "        [0.4926],\n",
       "        [0.4413],\n",
       "        [0.4366],\n",
       "        [0.4104],\n",
       "        [0.4248],\n",
       "        [0.4195],\n",
       "        [0.5359],\n",
       "        [0.3424],\n",
       "        [0.5233],\n",
       "        [0.5232],\n",
       "        [0.5008],\n",
       "        [0.4833],\n",
       "        [0.4984],\n",
       "        [0.4084],\n",
       "        [0.3976],\n",
       "        [0.5577],\n",
       "        [0.3899],\n",
       "        [0.4128],\n",
       "        [0.3366],\n",
       "        [0.3766],\n",
       "        [0.4831],\n",
       "        [0.4390],\n",
       "        [0.3756],\n",
       "        [0.6507],\n",
       "        [0.4385],\n",
       "        [0.4412],\n",
       "        [0.5199],\n",
       "        [0.4162],\n",
       "        [0.4148],\n",
       "        [0.5397],\n",
       "        [0.5539],\n",
       "        [0.6456],\n",
       "        [0.4883],\n",
       "        [0.4418],\n",
       "        [0.4826],\n",
       "        [0.3480],\n",
       "        [0.4986],\n",
       "        [0.3726],\n",
       "        [0.4349],\n",
       "        [0.5636],\n",
       "        [0.3747],\n",
       "        [0.5066],\n",
       "        [0.4200],\n",
       "        [0.6782],\n",
       "        [0.5329],\n",
       "        [0.4374],\n",
       "        [0.4039],\n",
       "        [0.6246],\n",
       "        [0.4537],\n",
       "        [0.4428],\n",
       "        [0.3621],\n",
       "        [0.4537],\n",
       "        [0.4300],\n",
       "        [0.4744],\n",
       "        [0.4446],\n",
       "        [0.4974],\n",
       "        [0.5165],\n",
       "        [0.3250],\n",
       "        [0.5076],\n",
       "        [0.4801],\n",
       "        [0.5077],\n",
       "        [0.6359],\n",
       "        [0.4797],\n",
       "        [0.4414],\n",
       "        [0.5426],\n",
       "        [0.4205],\n",
       "        [0.4567],\n",
       "        [0.4079],\n",
       "        [0.4707],\n",
       "        [0.4687],\n",
       "        [0.4508],\n",
       "        [0.3904],\n",
       "        [0.4865],\n",
       "        [0.5495],\n",
       "        [0.3780],\n",
       "        [0.4049],\n",
       "        [0.3638],\n",
       "        [0.4989],\n",
       "        [0.3488],\n",
       "        [0.4851],\n",
       "        [0.4821],\n",
       "        [0.3666],\n",
       "        [0.5263],\n",
       "        [0.5195],\n",
       "        [0.4029],\n",
       "        [0.3696],\n",
       "        [0.3338],\n",
       "        [0.4729],\n",
       "        [0.4109],\n",
       "        [0.3786],\n",
       "        [0.5309],\n",
       "        [0.4513],\n",
       "        [0.4054],\n",
       "        [0.5120],\n",
       "        [0.5499],\n",
       "        [0.4209],\n",
       "        [0.4699],\n",
       "        [0.5487],\n",
       "        [0.4816],\n",
       "        [0.4401],\n",
       "        [0.5786],\n",
       "        [0.5834],\n",
       "        [0.4256],\n",
       "        [0.4689],\n",
       "        [0.5027],\n",
       "        [0.4074],\n",
       "        [0.4305],\n",
       "        [0.5853],\n",
       "        [0.4839],\n",
       "        [0.4304],\n",
       "        [0.4195],\n",
       "        [0.6656],\n",
       "        [0.4911],\n",
       "        [0.6148],\n",
       "        [0.5042],\n",
       "        [0.3945],\n",
       "        [0.4070],\n",
       "        [0.7827],\n",
       "        [0.4162],\n",
       "        [0.6075],\n",
       "        [0.2957],\n",
       "        [0.3875],\n",
       "        [0.4388],\n",
       "        [0.3980],\n",
       "        [0.6091],\n",
       "        [0.5970],\n",
       "        [0.3706],\n",
       "        [0.4577],\n",
       "        [0.4304],\n",
       "        [0.4176],\n",
       "        [0.4256],\n",
       "        [0.4441],\n",
       "        [0.3896],\n",
       "        [0.4606],\n",
       "        [0.5625],\n",
       "        [0.4268],\n",
       "        [0.5166],\n",
       "        [0.3978],\n",
       "        [0.5200],\n",
       "        [0.5601],\n",
       "        [0.4167],\n",
       "        [0.4115],\n",
       "        [0.6994],\n",
       "        [0.4254],\n",
       "        [0.3769],\n",
       "        [0.4558],\n",
       "        [0.4637],\n",
       "        [0.5361],\n",
       "        [0.4597],\n",
       "        [0.5894],\n",
       "        [0.4352],\n",
       "        [0.4932],\n",
       "        [0.5012],\n",
       "        [0.3556],\n",
       "        [0.4596],\n",
       "        [0.4851],\n",
       "        [0.4317],\n",
       "        [0.5756],\n",
       "        [0.5163],\n",
       "        [0.4454],\n",
       "        [0.5807],\n",
       "        [0.3630],\n",
       "        [0.4387],\n",
       "        [0.3214],\n",
       "        [0.4659],\n",
       "        [0.5352],\n",
       "        [0.3812],\n",
       "        [0.4237],\n",
       "        [0.3947],\n",
       "        [0.5026],\n",
       "        [0.5542],\n",
       "        [0.3941],\n",
       "        [0.3809],\n",
       "        [0.3557],\n",
       "        [0.4472],\n",
       "        [0.5627],\n",
       "        [0.4818],\n",
       "        [0.5035],\n",
       "        [0.4544],\n",
       "        [0.4381],\n",
       "        [0.4304],\n",
       "        [0.4783],\n",
       "        [0.4395],\n",
       "        [0.4443],\n",
       "        [0.4462],\n",
       "        [0.4468],\n",
       "        [0.4129],\n",
       "        [0.6375],\n",
       "        [0.4474],\n",
       "        [0.4413],\n",
       "        [0.4091],\n",
       "        [0.4783],\n",
       "        [0.4060],\n",
       "        [0.5488],\n",
       "        [0.4846],\n",
       "        [0.4054],\n",
       "        [0.4969],\n",
       "        [0.4906],\n",
       "        [0.6362],\n",
       "        [0.4254],\n",
       "        [0.4169],\n",
       "        [0.3774],\n",
       "        [0.4869],\n",
       "        [0.3771]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(X_train_tensor.shape[1])\n",
    "model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "79a8ab26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0474, -0.0225,  0.1492, -0.1157, -0.0101, -0.0542, -0.0192, -0.1352,\n",
       "         -0.0011,  0.1687, -0.0011,  0.0087,  0.0094,  0.1668, -0.1603,  0.0178,\n",
       "         -0.0629, -0.1246,  0.0285,  0.0995, -0.1779, -0.0211, -0.0454,  0.0586,\n",
       "         -0.0961,  0.1018,  0.1433,  0.0181,  0.1060,  0.0151]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c25874cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1373], requires_grad=True)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8d756d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [1, 1]                    --\n",
       "├─Linear: 1-1                            [1, 1]                    31\n",
       "├─Sigmoid: 1-2                           [1, 1]                    --\n",
       "==========================================================================================\n",
       "Total params: 31\n",
       "Trainable params: 31\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, X_train_tensor.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c646d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(num_features, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(16, 8)\n",
    "        self.linear3 = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.loss = nn.BCELoss()\n",
    " \n",
    "    def forward(self, features):\n",
    "        out = self.linear1(features)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "    def loss(self, pred, target):\n",
    "        return self.loss(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "72a4b589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5858],\n",
       "        [0.5744],\n",
       "        [0.5736],\n",
       "        [0.6000],\n",
       "        [0.5750],\n",
       "        [0.6108],\n",
       "        [0.5751],\n",
       "        [0.5839],\n",
       "        [0.5688],\n",
       "        [0.5779],\n",
       "        [0.5756],\n",
       "        [0.5830],\n",
       "        [0.5597],\n",
       "        [0.5924],\n",
       "        [0.5759],\n",
       "        [0.5843],\n",
       "        [0.5783],\n",
       "        [0.5762],\n",
       "        [0.5753],\n",
       "        [0.5723],\n",
       "        [0.5748],\n",
       "        [0.5679],\n",
       "        [0.5699],\n",
       "        [0.5779],\n",
       "        [0.6085],\n",
       "        [0.5701],\n",
       "        [0.5860],\n",
       "        [0.5786],\n",
       "        [0.5795],\n",
       "        [0.5947],\n",
       "        [0.5487],\n",
       "        [0.5720],\n",
       "        [0.6472],\n",
       "        [0.5702],\n",
       "        [0.5728],\n",
       "        [0.5751],\n",
       "        [0.5797],\n",
       "        [0.5875],\n",
       "        [0.5908],\n",
       "        [0.5797],\n",
       "        [0.5762],\n",
       "        [0.5657],\n",
       "        [0.5738],\n",
       "        [0.5662],\n",
       "        [0.5699],\n",
       "        [0.5722],\n",
       "        [0.5970],\n",
       "        [0.5746],\n",
       "        [0.5904],\n",
       "        [0.5811],\n",
       "        [0.5893],\n",
       "        [0.5672],\n",
       "        [0.5697],\n",
       "        [0.5687],\n",
       "        [0.5764],\n",
       "        [0.5719],\n",
       "        [0.5903],\n",
       "        [0.5735],\n",
       "        [0.5658],\n",
       "        [0.5722],\n",
       "        [0.5879],\n",
       "        [0.5677],\n",
       "        [0.5783],\n",
       "        [0.5788],\n",
       "        [0.5781],\n",
       "        [0.5668],\n",
       "        [0.5790],\n",
       "        [0.5674],\n",
       "        [0.5664],\n",
       "        [0.5758],\n",
       "        [0.5896],\n",
       "        [0.5773],\n",
       "        [0.5929],\n",
       "        [0.5673],\n",
       "        [0.5679],\n",
       "        [0.5938],\n",
       "        [0.5709],\n",
       "        [0.5737],\n",
       "        [0.5625],\n",
       "        [0.5835],\n",
       "        [0.5621],\n",
       "        [0.5731],\n",
       "        [0.5679],\n",
       "        [0.5846],\n",
       "        [0.5624],\n",
       "        [0.5704],\n",
       "        [0.5908],\n",
       "        [0.5785],\n",
       "        [0.5818],\n",
       "        [0.5723],\n",
       "        [0.5844],\n",
       "        [0.5811],\n",
       "        [0.5952],\n",
       "        [0.5739],\n",
       "        [0.5755],\n",
       "        [0.5948],\n",
       "        [0.5791],\n",
       "        [0.5986],\n",
       "        [0.5673],\n",
       "        [0.5712],\n",
       "        [0.5848],\n",
       "        [0.5711],\n",
       "        [0.5761],\n",
       "        [0.5810],\n",
       "        [0.5698],\n",
       "        [0.5683],\n",
       "        [0.5844],\n",
       "        [0.5800],\n",
       "        [0.5745],\n",
       "        [0.5685],\n",
       "        [0.5870],\n",
       "        [0.5637],\n",
       "        [0.5644],\n",
       "        [0.5717],\n",
       "        [0.5653],\n",
       "        [0.5706],\n",
       "        [0.5891],\n",
       "        [0.5845],\n",
       "        [0.5748],\n",
       "        [0.5659],\n",
       "        [0.6165],\n",
       "        [0.5651],\n",
       "        [0.5903],\n",
       "        [0.5630],\n",
       "        [0.5926],\n",
       "        [0.5771],\n",
       "        [0.5539],\n",
       "        [0.5794],\n",
       "        [0.5723],\n",
       "        [0.5648],\n",
       "        [0.6067],\n",
       "        [0.5905],\n",
       "        [0.5743],\n",
       "        [0.5736],\n",
       "        [0.5789],\n",
       "        [0.5725],\n",
       "        [0.5796],\n",
       "        [0.5988],\n",
       "        [0.5848],\n",
       "        [0.5628],\n",
       "        [0.5761],\n",
       "        [0.5847],\n",
       "        [0.5959],\n",
       "        [0.5636],\n",
       "        [0.6179],\n",
       "        [0.5782],\n",
       "        [0.6015],\n",
       "        [0.5695],\n",
       "        [0.5442],\n",
       "        [0.5769],\n",
       "        [0.5674],\n",
       "        [0.5707],\n",
       "        [0.5757],\n",
       "        [0.5601],\n",
       "        [0.5819],\n",
       "        [0.5669],\n",
       "        [0.6024],\n",
       "        [0.5722],\n",
       "        [0.5613],\n",
       "        [0.5622],\n",
       "        [0.5688],\n",
       "        [0.5746],\n",
       "        [0.5688],\n",
       "        [0.5658],\n",
       "        [0.5617],\n",
       "        [0.5959],\n",
       "        [0.5755],\n",
       "        [0.5633],\n",
       "        [0.5788],\n",
       "        [0.5642],\n",
       "        [0.5680],\n",
       "        [0.5734],\n",
       "        [0.5819],\n",
       "        [0.5839],\n",
       "        [0.5899],\n",
       "        [0.5923],\n",
       "        [0.5974],\n",
       "        [0.5696],\n",
       "        [0.5787],\n",
       "        [0.5801],\n",
       "        [0.5678],\n",
       "        [0.5754],\n",
       "        [0.5659],\n",
       "        [0.5769],\n",
       "        [0.5842],\n",
       "        [0.5829],\n",
       "        [0.5729],\n",
       "        [0.5872],\n",
       "        [0.5758],\n",
       "        [0.5728],\n",
       "        [0.5836],\n",
       "        [0.5667],\n",
       "        [0.5922],\n",
       "        [0.5856],\n",
       "        [0.5901],\n",
       "        [0.5578],\n",
       "        [0.5834],\n",
       "        [0.5764],\n",
       "        [0.5718],\n",
       "        [0.5882],\n",
       "        [0.5621],\n",
       "        [0.5799],\n",
       "        [0.5671],\n",
       "        [0.5667],\n",
       "        [0.5737],\n",
       "        [0.5716],\n",
       "        [0.5660],\n",
       "        [0.5966],\n",
       "        [0.5949],\n",
       "        [0.5689],\n",
       "        [0.5806],\n",
       "        [0.5719],\n",
       "        [0.5788],\n",
       "        [0.5758],\n",
       "        [0.5885],\n",
       "        [0.5687],\n",
       "        [0.5665],\n",
       "        [0.5733],\n",
       "        [0.5764],\n",
       "        [0.5544],\n",
       "        [0.5722],\n",
       "        [0.5990],\n",
       "        [0.5866],\n",
       "        [0.5844],\n",
       "        [0.5786],\n",
       "        [0.5726],\n",
       "        [0.5851],\n",
       "        [0.5692],\n",
       "        [0.5774],\n",
       "        [0.5860],\n",
       "        [0.5756],\n",
       "        [0.5731],\n",
       "        [0.5571],\n",
       "        [0.5714],\n",
       "        [0.5927],\n",
       "        [0.5983],\n",
       "        [0.5701],\n",
       "        [0.6080],\n",
       "        [0.5723],\n",
       "        [0.5738],\n",
       "        [0.5754],\n",
       "        [0.5684],\n",
       "        [0.5733],\n",
       "        [0.5876],\n",
       "        [0.5700],\n",
       "        [0.5918],\n",
       "        [0.5972],\n",
       "        [0.5733],\n",
       "        [0.5846],\n",
       "        [0.5697],\n",
       "        [0.5737],\n",
       "        [0.5775],\n",
       "        [0.5721],\n",
       "        [0.5672],\n",
       "        [0.5935],\n",
       "        [0.5838],\n",
       "        [0.5785],\n",
       "        [0.5797],\n",
       "        [0.5646],\n",
       "        [0.5668],\n",
       "        [0.5853],\n",
       "        [0.5983],\n",
       "        [0.5764],\n",
       "        [0.5676],\n",
       "        [0.5836],\n",
       "        [0.5694],\n",
       "        [0.5737],\n",
       "        [0.5790],\n",
       "        [0.5904],\n",
       "        [0.5851],\n",
       "        [0.5793],\n",
       "        [0.5818],\n",
       "        [0.5725],\n",
       "        [0.5763],\n",
       "        [0.6003],\n",
       "        [0.5815],\n",
       "        [0.5713],\n",
       "        [0.5552],\n",
       "        [0.5750],\n",
       "        [0.5733],\n",
       "        [0.5675],\n",
       "        [0.5950],\n",
       "        [0.5939],\n",
       "        [0.5849],\n",
       "        [0.5838],\n",
       "        [0.5684],\n",
       "        [0.5816],\n",
       "        [0.5785],\n",
       "        [0.5772],\n",
       "        [0.5907],\n",
       "        [0.5835],\n",
       "        [0.5695],\n",
       "        [0.5699],\n",
       "        [0.5734],\n",
       "        [0.6000],\n",
       "        [0.5837],\n",
       "        [0.5687],\n",
       "        [0.5690],\n",
       "        [0.5840],\n",
       "        [0.5598],\n",
       "        [0.5824],\n",
       "        [0.5758],\n",
       "        [0.5905],\n",
       "        [0.5727],\n",
       "        [0.5699],\n",
       "        [0.5885],\n",
       "        [0.5768],\n",
       "        [0.5866],\n",
       "        [0.5703],\n",
       "        [0.5697],\n",
       "        [0.5762],\n",
       "        [0.5583],\n",
       "        [0.6045],\n",
       "        [0.5717],\n",
       "        [0.5815],\n",
       "        [0.6128],\n",
       "        [0.5697],\n",
       "        [0.5729],\n",
       "        [0.5745],\n",
       "        [0.5821],\n",
       "        [0.5740],\n",
       "        [0.5786],\n",
       "        [0.5658],\n",
       "        [0.5851],\n",
       "        [0.5677],\n",
       "        [0.5884],\n",
       "        [0.5754],\n",
       "        [0.5770],\n",
       "        [0.5664],\n",
       "        [0.5691],\n",
       "        [0.5731],\n",
       "        [0.5733],\n",
       "        [0.5792],\n",
       "        [0.5982],\n",
       "        [0.5750],\n",
       "        [0.5787],\n",
       "        [0.5832],\n",
       "        [0.5846],\n",
       "        [0.5839],\n",
       "        [0.5691],\n",
       "        [0.5901],\n",
       "        [0.6149],\n",
       "        [0.5737],\n",
       "        [0.5713],\n",
       "        [0.5681],\n",
       "        [0.5792],\n",
       "        [0.5688],\n",
       "        [0.5720],\n",
       "        [0.5880],\n",
       "        [0.5841],\n",
       "        [0.5781],\n",
       "        [0.6071],\n",
       "        [0.5678],\n",
       "        [0.5920],\n",
       "        [0.5650],\n",
       "        [0.5650],\n",
       "        [0.5671],\n",
       "        [0.5753],\n",
       "        [0.5683],\n",
       "        [0.5617],\n",
       "        [0.5824],\n",
       "        [0.5735],\n",
       "        [0.6204],\n",
       "        [0.5763],\n",
       "        [0.5835],\n",
       "        [0.5659],\n",
       "        [0.5749],\n",
       "        [0.6018],\n",
       "        [0.6437],\n",
       "        [0.5695],\n",
       "        [0.5683],\n",
       "        [0.5836],\n",
       "        [0.5731],\n",
       "        [0.5847],\n",
       "        [0.5672],\n",
       "        [0.6073],\n",
       "        [0.6214],\n",
       "        [0.5631],\n",
       "        [0.5772],\n",
       "        [0.5794],\n",
       "        [0.5831],\n",
       "        [0.5727],\n",
       "        [0.5966],\n",
       "        [0.5807],\n",
       "        [0.5847],\n",
       "        [0.5767],\n",
       "        [0.5878],\n",
       "        [0.5990],\n",
       "        [0.5703],\n",
       "        [0.5660],\n",
       "        [0.5701],\n",
       "        [0.5828],\n",
       "        [0.5678],\n",
       "        [0.6213],\n",
       "        [0.5597],\n",
       "        [0.5705],\n",
       "        [0.5725],\n",
       "        [0.5717],\n",
       "        [0.6022],\n",
       "        [0.5649],\n",
       "        [0.5934],\n",
       "        [0.5837],\n",
       "        [0.5655],\n",
       "        [0.5763],\n",
       "        [0.5832],\n",
       "        [0.5779],\n",
       "        [0.5873],\n",
       "        [0.5690],\n",
       "        [0.6115],\n",
       "        [0.5675],\n",
       "        [0.5787],\n",
       "        [0.5988],\n",
       "        [0.5757],\n",
       "        [0.5729],\n",
       "        [0.5736],\n",
       "        [0.5833],\n",
       "        [0.5950],\n",
       "        [0.5609],\n",
       "        [0.5666],\n",
       "        [0.5782],\n",
       "        [0.5796],\n",
       "        [0.6046],\n",
       "        [0.5771],\n",
       "        [0.5689],\n",
       "        [0.5701],\n",
       "        [0.5675],\n",
       "        [0.5587],\n",
       "        [0.5740],\n",
       "        [0.5656],\n",
       "        [0.5806],\n",
       "        [0.5863],\n",
       "        [0.5729],\n",
       "        [0.5668],\n",
       "        [0.5771],\n",
       "        [0.5783],\n",
       "        [0.5838],\n",
       "        [0.5619],\n",
       "        [0.5721],\n",
       "        [0.5937],\n",
       "        [0.5753],\n",
       "        [0.5734],\n",
       "        [0.5750],\n",
       "        [0.5761],\n",
       "        [0.5849],\n",
       "        [0.6031],\n",
       "        [0.5786],\n",
       "        [0.5605],\n",
       "        [0.5750],\n",
       "        [0.5674],\n",
       "        [0.5578],\n",
       "        [0.5661],\n",
       "        [0.5831],\n",
       "        [0.5849],\n",
       "        [0.5787],\n",
       "        [0.5670]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(X_train_tensor.shape[1])\n",
    "model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9dd56f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [1, 1]                    --\n",
       "├─Linear: 1-1                            [1, 16]                   496\n",
       "├─ReLU: 1-2                              [1, 16]                   --\n",
       "├─Linear: 1-3                            [1, 8]                    136\n",
       "├─ReLU: 1-4                              [1, 8]                    --\n",
       "├─Linear: 1-5                            [1, 1]                    9\n",
       "├─Sigmoid: 1-6                           [1, 1]                    --\n",
       "==========================================================================================\n",
       "Total params: 641\n",
       "Trainable params: 641\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, X_train_tensor.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "94249f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1431, -0.1024, -0.1019,  0.0380, -0.0035, -0.0248, -0.0536,  0.0995,\n",
       "          0.1822,  0.0943, -0.1582, -0.0642, -0.1320,  0.0257, -0.0556, -0.1643,\n",
       "         -0.0509, -0.0042, -0.0232, -0.0246, -0.0339, -0.0110,  0.0352,  0.1629,\n",
       "          0.0674, -0.0888, -0.0648,  0.0834, -0.0495, -0.0694],\n",
       "        [-0.0706, -0.1764,  0.0485, -0.1763,  0.0957,  0.1450, -0.1709,  0.0914,\n",
       "         -0.0494,  0.0445,  0.1712, -0.1580, -0.1749, -0.1439,  0.1028,  0.0709,\n",
       "         -0.0548, -0.1494, -0.0474,  0.1005, -0.1451,  0.1805, -0.0912, -0.0987,\n",
       "          0.0253,  0.1486, -0.0066,  0.1622, -0.1357, -0.0958],\n",
       "        [-0.1097,  0.0111, -0.0444, -0.0425,  0.1194, -0.1726,  0.0404,  0.0797,\n",
       "         -0.1062,  0.0200, -0.1666, -0.0631, -0.0584, -0.0569,  0.1392, -0.0562,\n",
       "          0.1482,  0.0069, -0.0653,  0.0859, -0.0264, -0.0980, -0.0040,  0.1805,\n",
       "          0.0699,  0.0665,  0.0580, -0.0423,  0.0810, -0.1482],\n",
       "        [-0.1790,  0.1328, -0.1706,  0.1360, -0.1786,  0.1142,  0.0514,  0.1078,\n",
       "         -0.0790, -0.0817, -0.0768,  0.0901,  0.0468,  0.1743, -0.1109, -0.0340,\n",
       "          0.1341, -0.0803, -0.0048, -0.0476,  0.0461, -0.0665, -0.0418,  0.0926,\n",
       "         -0.0893,  0.0724,  0.0653,  0.1559,  0.1493,  0.1656],\n",
       "        [ 0.1328,  0.1436, -0.0107,  0.1457, -0.0974, -0.1563, -0.1789, -0.0721,\n",
       "         -0.0980,  0.0250, -0.0601, -0.0831,  0.1767, -0.0265,  0.0774, -0.1556,\n",
       "         -0.1117,  0.1205,  0.0422,  0.0476, -0.0274,  0.1519, -0.1090,  0.0954,\n",
       "         -0.0724,  0.0583, -0.0710, -0.1094, -0.0146, -0.0118],\n",
       "        [-0.0348, -0.0005,  0.1758, -0.1331, -0.0563, -0.0217,  0.1334,  0.0945,\n",
       "         -0.0086, -0.0123, -0.1410,  0.1018, -0.1231,  0.1805,  0.0942, -0.1019,\n",
       "         -0.1334,  0.0452, -0.1526,  0.0765,  0.1367, -0.0290,  0.0960,  0.1673,\n",
       "         -0.0585, -0.1322, -0.0514,  0.1214, -0.0005, -0.0225],\n",
       "        [-0.1319, -0.0040, -0.1128,  0.0831, -0.1754, -0.0785, -0.1708,  0.1317,\n",
       "         -0.1140,  0.0459, -0.1248, -0.0742, -0.0780,  0.1528, -0.0006, -0.1655,\n",
       "          0.1402, -0.0106, -0.1093,  0.1456,  0.1462,  0.1230, -0.0833, -0.0917,\n",
       "          0.1733, -0.1440,  0.0429, -0.1513,  0.0903,  0.1401],\n",
       "        [-0.0374,  0.0634, -0.1415, -0.0434, -0.1082, -0.1528,  0.0913,  0.1730,\n",
       "         -0.0727,  0.1003, -0.0514, -0.0002, -0.0016,  0.1391,  0.0743, -0.0396,\n",
       "          0.1760, -0.1527,  0.0747,  0.0714,  0.1764, -0.1514,  0.0249,  0.0661,\n",
       "          0.1763, -0.0968,  0.1357, -0.0662,  0.1788,  0.1564],\n",
       "        [-0.0475, -0.1294,  0.1201, -0.0132,  0.1367,  0.0136, -0.0624, -0.0982,\n",
       "         -0.0257, -0.1557, -0.1791, -0.0874,  0.1277,  0.0498,  0.0318,  0.1206,\n",
       "         -0.1167,  0.0323, -0.1630, -0.1701,  0.1363,  0.0558,  0.1665, -0.1085,\n",
       "         -0.0194,  0.0608,  0.0558, -0.1634, -0.0669,  0.1712],\n",
       "        [ 0.0712,  0.0817,  0.1150,  0.0498,  0.1731, -0.1346,  0.1566,  0.0924,\n",
       "         -0.1645,  0.1211,  0.0862,  0.1158, -0.0787, -0.1464, -0.0895, -0.0339,\n",
       "          0.1144, -0.1747, -0.0909,  0.0675,  0.0405,  0.1216, -0.0187, -0.0221,\n",
       "          0.1128,  0.1373,  0.0572,  0.1135,  0.0657,  0.1247],\n",
       "        [-0.1119,  0.0056,  0.1421,  0.0017,  0.0619,  0.1611, -0.1034, -0.0823,\n",
       "         -0.0432,  0.0387,  0.0534,  0.1231,  0.1181, -0.0328, -0.0819, -0.1360,\n",
       "         -0.1676, -0.0305, -0.0010,  0.1720,  0.0210, -0.1727,  0.0682, -0.0424,\n",
       "          0.1716,  0.1261, -0.0078, -0.1110,  0.0549, -0.0191],\n",
       "        [ 0.1424,  0.1609,  0.0242,  0.0116,  0.1224, -0.0781,  0.1642, -0.0990,\n",
       "          0.1097, -0.0486,  0.0324, -0.0947, -0.0598, -0.0345,  0.1669,  0.0455,\n",
       "         -0.1269, -0.1507, -0.0120, -0.0029, -0.1463,  0.0741, -0.1021, -0.0310,\n",
       "          0.0975, -0.1692,  0.0181,  0.1690, -0.0623,  0.1043],\n",
       "        [ 0.0766, -0.1187, -0.0342,  0.1174, -0.1495, -0.1800,  0.1316,  0.0237,\n",
       "         -0.0562,  0.0387, -0.0588,  0.0806, -0.0993, -0.1496, -0.0420, -0.1233,\n",
       "          0.1615,  0.1333, -0.1054,  0.1139,  0.0180, -0.0468, -0.1631,  0.0525,\n",
       "         -0.0673,  0.0010, -0.0589,  0.1814,  0.1377, -0.0383],\n",
       "        [-0.0270,  0.0682,  0.0184, -0.1074,  0.0904,  0.0309,  0.0874,  0.1767,\n",
       "         -0.1183, -0.0588,  0.1282, -0.1371, -0.0262, -0.0823, -0.1565, -0.1136,\n",
       "         -0.0249,  0.1811, -0.1382, -0.0637, -0.1779, -0.0077, -0.1693, -0.1575,\n",
       "         -0.0236, -0.1694, -0.1173, -0.0550, -0.0392,  0.1238],\n",
       "        [ 0.0813,  0.0631,  0.0069, -0.0531, -0.0590, -0.0853,  0.1643, -0.1157,\n",
       "         -0.1749, -0.0080,  0.0671,  0.0489,  0.1218,  0.0230, -0.0623,  0.0187,\n",
       "          0.0535, -0.0264, -0.0597,  0.1639, -0.0662, -0.0287, -0.0892,  0.0110,\n",
       "         -0.0770, -0.0055, -0.1405, -0.1781, -0.1113, -0.1766],\n",
       "        [-0.0374,  0.0877, -0.1571,  0.1310,  0.0846,  0.0703, -0.0512, -0.0987,\n",
       "          0.0842,  0.1621,  0.0130,  0.1442,  0.0696,  0.0256, -0.0027, -0.0026,\n",
       "          0.0749,  0.0246, -0.0440,  0.0117,  0.1401, -0.0748, -0.1319,  0.1470,\n",
       "         -0.1132, -0.1246, -0.0227,  0.0064, -0.1320,  0.0749]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7115ec8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b34f6f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear1.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "50327192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(num_features, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    " \n",
    "    def forward(self, features):\n",
    "        out = self.network(features)\n",
    "        \n",
    "        return out\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c2e6254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4516],\n",
       "        [0.4606],\n",
       "        [0.4696],\n",
       "        [0.4026],\n",
       "        [0.4103],\n",
       "        [0.2140],\n",
       "        [0.4608],\n",
       "        [0.4714],\n",
       "        [0.4673],\n",
       "        [0.4286],\n",
       "        [0.4050],\n",
       "        [0.3250],\n",
       "        [0.4474],\n",
       "        [0.4572],\n",
       "        [0.4597],\n",
       "        [0.4630],\n",
       "        [0.4611],\n",
       "        [0.4479],\n",
       "        [0.4576],\n",
       "        [0.4521],\n",
       "        [0.4597],\n",
       "        [0.4610],\n",
       "        [0.4372],\n",
       "        [0.4137],\n",
       "        [0.2429],\n",
       "        [0.4518],\n",
       "        [0.4608],\n",
       "        [0.4553],\n",
       "        [0.4035],\n",
       "        [0.3558],\n",
       "        [0.4678],\n",
       "        [0.4238],\n",
       "        [0.3073],\n",
       "        [0.4291],\n",
       "        [0.4574],\n",
       "        [0.4570],\n",
       "        [0.4679],\n",
       "        [0.4630],\n",
       "        [0.4308],\n",
       "        [0.4696],\n",
       "        [0.4824],\n",
       "        [0.4623],\n",
       "        [0.4521],\n",
       "        [0.4765],\n",
       "        [0.4568],\n",
       "        [0.4542],\n",
       "        [0.3475],\n",
       "        [0.4581],\n",
       "        [0.4256],\n",
       "        [0.4479],\n",
       "        [0.4365],\n",
       "        [0.4528],\n",
       "        [0.4615],\n",
       "        [0.4749],\n",
       "        [0.4557],\n",
       "        [0.3925],\n",
       "        [0.4551],\n",
       "        [0.4325],\n",
       "        [0.4576],\n",
       "        [0.4634],\n",
       "        [0.4283],\n",
       "        [0.4577],\n",
       "        [0.4586],\n",
       "        [0.4525],\n",
       "        [0.4535],\n",
       "        [0.4758],\n",
       "        [0.4789],\n",
       "        [0.4632],\n",
       "        [0.4348],\n",
       "        [0.4682],\n",
       "        [0.4227],\n",
       "        [0.4759],\n",
       "        [0.4034],\n",
       "        [0.4549],\n",
       "        [0.4409],\n",
       "        [0.4333],\n",
       "        [0.4621],\n",
       "        [0.4578],\n",
       "        [0.4687],\n",
       "        [0.4688],\n",
       "        [0.4542],\n",
       "        [0.4612],\n",
       "        [0.4661],\n",
       "        [0.4416],\n",
       "        [0.4623],\n",
       "        [0.4755],\n",
       "        [0.4645],\n",
       "        [0.4259],\n",
       "        [0.4692],\n",
       "        [0.4508],\n",
       "        [0.4635],\n",
       "        [0.4518],\n",
       "        [0.3965],\n",
       "        [0.4641],\n",
       "        [0.4733],\n",
       "        [0.2652],\n",
       "        [0.4700],\n",
       "        [0.4425],\n",
       "        [0.4603],\n",
       "        [0.4671],\n",
       "        [0.4634],\n",
       "        [0.4378],\n",
       "        [0.3913],\n",
       "        [0.4455],\n",
       "        [0.4568],\n",
       "        [0.4519],\n",
       "        [0.4291],\n",
       "        [0.4468],\n",
       "        [0.4688],\n",
       "        [0.4059],\n",
       "        [0.3345],\n",
       "        [0.4651],\n",
       "        [0.4627],\n",
       "        [0.4532],\n",
       "        [0.4669],\n",
       "        [0.4414],\n",
       "        [0.4164],\n",
       "        [0.4670],\n",
       "        [0.4657],\n",
       "        [0.4192],\n",
       "        [0.3500],\n",
       "        [0.4615],\n",
       "        [0.3888],\n",
       "        [0.4661],\n",
       "        [0.3713],\n",
       "        [0.4525],\n",
       "        [0.4096],\n",
       "        [0.4473],\n",
       "        [0.4029],\n",
       "        [0.4048],\n",
       "        [0.4309],\n",
       "        [0.3881],\n",
       "        [0.4630],\n",
       "        [0.4667],\n",
       "        [0.4676],\n",
       "        [0.4649],\n",
       "        [0.3943],\n",
       "        [0.4178],\n",
       "        [0.4107],\n",
       "        [0.4625],\n",
       "        [0.4668],\n",
       "        [0.4732],\n",
       "        [0.4499],\n",
       "        [0.4652],\n",
       "        [0.3774],\n",
       "        [0.3966],\n",
       "        [0.3796],\n",
       "        [0.4609],\n",
       "        [0.3243],\n",
       "        [0.4492],\n",
       "        [0.4547],\n",
       "        [0.4739],\n",
       "        [0.4489],\n",
       "        [0.4631],\n",
       "        [0.4516],\n",
       "        [0.4583],\n",
       "        [0.4171],\n",
       "        [0.4551],\n",
       "        [0.4694],\n",
       "        [0.4619],\n",
       "        [0.4566],\n",
       "        [0.4526],\n",
       "        [0.4647],\n",
       "        [0.4617],\n",
       "        [0.4594],\n",
       "        [0.4148],\n",
       "        [0.4710],\n",
       "        [0.4732],\n",
       "        [0.4582],\n",
       "        [0.4622],\n",
       "        [0.3794],\n",
       "        [0.4286],\n",
       "        [0.4466],\n",
       "        [0.4701],\n",
       "        [0.4687],\n",
       "        [0.4443],\n",
       "        [0.4040],\n",
       "        [0.4617],\n",
       "        [0.3827],\n",
       "        [0.4312],\n",
       "        [0.4674],\n",
       "        [0.4521],\n",
       "        [0.4640],\n",
       "        [0.3061],\n",
       "        [0.4558],\n",
       "        [0.3959],\n",
       "        [0.4663],\n",
       "        [0.4067],\n",
       "        [0.4491],\n",
       "        [0.4168],\n",
       "        [0.4464],\n",
       "        [0.4457],\n",
       "        [0.4554],\n",
       "        [0.3930],\n",
       "        [0.4501],\n",
       "        [0.4458],\n",
       "        [0.4257],\n",
       "        [0.4566],\n",
       "        [0.4665],\n",
       "        [0.4114],\n",
       "        [0.4602],\n",
       "        [0.4573],\n",
       "        [0.4521],\n",
       "        [0.4695],\n",
       "        [0.4620],\n",
       "        [0.4680],\n",
       "        [0.4682],\n",
       "        [0.3913],\n",
       "        [0.4310],\n",
       "        [0.3312],\n",
       "        [0.4725],\n",
       "        [0.4594],\n",
       "        [0.4452],\n",
       "        [0.4713],\n",
       "        [0.3922],\n",
       "        [0.4629],\n",
       "        [0.4520],\n",
       "        [0.4714],\n",
       "        [0.4660],\n",
       "        [0.3795],\n",
       "        [0.4704],\n",
       "        [0.3562],\n",
       "        [0.4666],\n",
       "        [0.4710],\n",
       "        [0.4276],\n",
       "        [0.4604],\n",
       "        [0.4175],\n",
       "        [0.4717],\n",
       "        [0.4730],\n",
       "        [0.4003],\n",
       "        [0.4458],\n",
       "        [0.4623],\n",
       "        [0.4554],\n",
       "        [0.4654],\n",
       "        [0.4420],\n",
       "        [0.4117],\n",
       "        [0.4614],\n",
       "        [0.4258],\n",
       "        [0.4740],\n",
       "        [0.4542],\n",
       "        [0.4615],\n",
       "        [0.4681],\n",
       "        [0.4582],\n",
       "        [0.4628],\n",
       "        [0.4646],\n",
       "        [0.3903],\n",
       "        [0.3707],\n",
       "        [0.4535],\n",
       "        [0.4132],\n",
       "        [0.4352],\n",
       "        [0.4583],\n",
       "        [0.4729],\n",
       "        [0.4327],\n",
       "        [0.4648],\n",
       "        [0.4759],\n",
       "        [0.4516],\n",
       "        [0.4295],\n",
       "        [0.4569],\n",
       "        [0.4553],\n",
       "        [0.4717],\n",
       "        [0.4432],\n",
       "        [0.4452],\n",
       "        [0.4381],\n",
       "        [0.4584],\n",
       "        [0.4707],\n",
       "        [0.4529],\n",
       "        [0.4536],\n",
       "        [0.4278],\n",
       "        [0.3527],\n",
       "        [0.4087],\n",
       "        [0.4428],\n",
       "        [0.4585],\n",
       "        [0.4616],\n",
       "        [0.4640],\n",
       "        [0.3715],\n",
       "        [0.4261],\n",
       "        [0.4537],\n",
       "        [0.4729],\n",
       "        [0.4652],\n",
       "        [0.4523],\n",
       "        [0.4551],\n",
       "        [0.4454],\n",
       "        [0.4473],\n",
       "        [0.3607],\n",
       "        [0.4201],\n",
       "        [0.4611],\n",
       "        [0.4281],\n",
       "        [0.4638],\n",
       "        [0.3970],\n",
       "        [0.4032],\n",
       "        [0.4543],\n",
       "        [0.3807],\n",
       "        [0.4699],\n",
       "        [0.4206],\n",
       "        [0.4119],\n",
       "        [0.4549],\n",
       "        [0.4345],\n",
       "        [0.4553],\n",
       "        [0.3636],\n",
       "        [0.4594],\n",
       "        [0.4269],\n",
       "        [0.3555],\n",
       "        [0.4659],\n",
       "        [0.4564],\n",
       "        [0.4619],\n",
       "        [0.4413],\n",
       "        [0.3760],\n",
       "        [0.3848],\n",
       "        [0.4785],\n",
       "        [0.4641],\n",
       "        [0.4667],\n",
       "        [0.4614],\n",
       "        [0.4110],\n",
       "        [0.4666],\n",
       "        [0.4406],\n",
       "        [0.3482],\n",
       "        [0.4662],\n",
       "        [0.4588],\n",
       "        [0.4543],\n",
       "        [0.4399],\n",
       "        [0.4544],\n",
       "        [0.4526],\n",
       "        [0.4231],\n",
       "        [0.4631],\n",
       "        [0.4672],\n",
       "        [0.4205],\n",
       "        [0.4338],\n",
       "        [0.4380],\n",
       "        [0.4677],\n",
       "        [0.4194],\n",
       "        [0.4705],\n",
       "        [0.4583],\n",
       "        [0.4102],\n",
       "        [0.4593],\n",
       "        [0.4547],\n",
       "        [0.4471],\n",
       "        [0.4088],\n",
       "        [0.3741],\n",
       "        [0.4344],\n",
       "        [0.4558],\n",
       "        [0.4424],\n",
       "        [0.3903],\n",
       "        [0.4677],\n",
       "        [0.4615],\n",
       "        [0.4684],\n",
       "        [0.4270],\n",
       "        [0.4713],\n",
       "        [0.4620],\n",
       "        [0.4090],\n",
       "        [0.4373],\n",
       "        [0.4591],\n",
       "        [0.4306],\n",
       "        [0.4587],\n",
       "        [0.4694],\n",
       "        [0.4840],\n",
       "        [0.4691],\n",
       "        [0.4699],\n",
       "        [0.4416],\n",
       "        [0.3136],\n",
       "        [0.4637],\n",
       "        [0.4453],\n",
       "        [0.4587],\n",
       "        [0.3923],\n",
       "        [0.4607],\n",
       "        [0.4560],\n",
       "        [0.4657],\n",
       "        [0.4661],\n",
       "        [0.4333],\n",
       "        [0.3207],\n",
       "        [0.4668],\n",
       "        [0.4652],\n",
       "        [0.4224],\n",
       "        [0.4574],\n",
       "        [0.3744],\n",
       "        [0.4415],\n",
       "        [0.3914],\n",
       "        [0.2804],\n",
       "        [0.4422],\n",
       "        [0.4525],\n",
       "        [0.4622],\n",
       "        [0.4133],\n",
       "        [0.4662],\n",
       "        [0.3127],\n",
       "        [0.4329],\n",
       "        [0.3963],\n",
       "        [0.4628],\n",
       "        [0.4658],\n",
       "        [0.3792],\n",
       "        [0.4475],\n",
       "        [0.4678],\n",
       "        [0.4694],\n",
       "        [0.4545],\n",
       "        [0.4710],\n",
       "        [0.3710],\n",
       "        [0.4663],\n",
       "        [0.4619],\n",
       "        [0.4556],\n",
       "        [0.4577],\n",
       "        [0.4083],\n",
       "        [0.4668],\n",
       "        [0.4606],\n",
       "        [0.4472],\n",
       "        [0.4710],\n",
       "        [0.4522],\n",
       "        [0.3756],\n",
       "        [0.4597],\n",
       "        [0.4198],\n",
       "        [0.4716],\n",
       "        [0.3298],\n",
       "        [0.4666],\n",
       "        [0.4598],\n",
       "        [0.4308],\n",
       "        [0.4109],\n",
       "        [0.4558],\n",
       "        [0.4328],\n",
       "        [0.4644],\n",
       "        [0.4531],\n",
       "        [0.4540],\n",
       "        [0.4528],\n",
       "        [0.4092],\n",
       "        [0.4653],\n",
       "        [0.3614],\n",
       "        [0.4630],\n",
       "        [0.4690],\n",
       "        [0.4535],\n",
       "        [0.4632],\n",
       "        [0.4436],\n",
       "        [0.4605],\n",
       "        [0.4648],\n",
       "        [0.4442],\n",
       "        [0.3989],\n",
       "        [0.4650],\n",
       "        [0.4558],\n",
       "        [0.4529],\n",
       "        [0.4672],\n",
       "        [0.4265],\n",
       "        [0.4635],\n",
       "        [0.4550],\n",
       "        [0.3965],\n",
       "        [0.4743],\n",
       "        [0.4710],\n",
       "        [0.4637],\n",
       "        [0.4370],\n",
       "        [0.4681],\n",
       "        [0.4145],\n",
       "        [0.4567],\n",
       "        [0.4658],\n",
       "        [0.4564],\n",
       "        [0.4607],\n",
       "        [0.4585],\n",
       "        [0.4586],\n",
       "        [0.4614],\n",
       "        [0.4608],\n",
       "        [0.4735],\n",
       "        [0.4204]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(X_train_tensor.shape[1])\n",
    "model(X_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a3e592be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [1, 1]                    --\n",
       "├─Sequential: 1-1                        [1, 1]                    --\n",
       "│    └─Linear: 2-1                       [1, 16]                   496\n",
       "│    └─ReLU: 2-2                         [1, 16]                   --\n",
       "│    └─Linear: 2-3                       [1, 8]                    136\n",
       "│    └─ReLU: 2-4                         [1, 8]                    --\n",
       "│    └─Linear: 2-5                       [1, 1]                    9\n",
       "│    └─Sigmoid: 2-6                      [1, 1]                    --\n",
       "==========================================================================================\n",
       "Total params: 641\n",
       "Trainable params: 641\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, X_train_tensor.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cab3c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ed2a2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ef7104fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.7096940875053406\n",
      "Epoch 2 Loss: 0.5018615126609802\n",
      "Epoch 3 Loss: 0.2121233195066452\n",
      "Epoch 4 Loss: 0.1400049924850464\n",
      "Epoch 5 Loss: 0.11993232369422913\n",
      "Epoch 6 Loss: 0.10215137898921967\n",
      "Epoch 7 Loss: 0.08759744465351105\n",
      "Epoch 8 Loss: 0.08766462653875351\n",
      "Epoch 9 Loss: 0.07885690778493881\n",
      "Epoch 10 Loss: 0.06446459889411926\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(epochs):\n",
    "    pred = model(X_train_tensor)\n",
    "    loss = loss_function(pred.view(-1), y_train_tensor.view(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "    print(f'Epoch {epochs+1} Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "71541053",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred = (y_pred > 0.7).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "540108c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (y_pred == y_test_tensor).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d781b6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5416)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
